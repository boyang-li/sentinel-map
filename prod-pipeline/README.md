# Production Pipeline - Ingestion Layer

High-performance Golang Kafka producer for streaming traffic sign detection metadata.

## Overview

This module bridges offline YOLOv8 detections with real-time cloud analytics:

1. **Reads** CSV detection files from perception layer
2. **Enriches** metadata (UUIDs, timestamps, vehicle/session IDs)
3. **Streams** to Kafka using goroutine-based parallelism
4. **Guarantees** delivery with exponential backoff + idempotent writes
5. **Reports** real-time TPS metrics

**Pipeline**: 256GB video ‚Üí YOLOv8 + GPS extraction ‚Üí CSV (500K records) ‚Üí **This Module** (5K-15K msg/sec) ‚Üí Kafka ‚Üí Snowflake

**Note**: GPS coordinates are extracted from frame image EXIF metadata or dashcam telemetry during perception layer processing.

---

## Data Format

### Input: CSV (from YOLOv8 + Frame Metadata)
```csv
frame_number,timestamp_sec,u,v,confidence,class_name,vehicle_lat,vehicle_lon,heading
75,2.500,1737.28,630.06,0.5249,stop sign,43.7900,-79.3140,45.0
185,6.167,2141.59,200.01,0.3381,traffic light,43.7905,-79.3138,47.5
```

**Columns**:
- `frame_number`: Video frame index
- `timestamp_sec`: Video timestamp in seconds
- `u`, `v`: Pixel coordinates (bottom-center of bounding box)
- `confidence`: YOLO confidence score (0-1)
- `class_name`: Object class ("stop sign" or "traffic light")
- `vehicle_lat`, `vehicle_lon`: GPS coordinates from frame image metadata
- `heading`: Vehicle heading in degrees (0¬∞=North)

### Output: Kafka JSON
```json
{
  "detection_id": "550e8400-e29b-41d4-a716-446655440000",
  "vehicle_id": "vehicle-001",
  "session_id": "20260119-abc123",
  "ingested_at": "2026-01-19T10:30:45Z",
  "frame_number": 75,
  "timestamp_sec": 2.5,
  "pixel_u": 1737.28,
  "pixel_v": 630.06,
  "confidence": 0.5249,
  "class_name": "stop sign",
  "vehicle_lat": 43.7900,
  "vehicle_lon": -79.3140,
  "heading": 45.0
}
```

**Note**: GPS coordinates are extracted from frame image EXIF metadata during perception layer processing.
```

---

## Quick Start

### Test Without Kafka (Dry-Run)
```bash
cd prod-pipeline
make deps          # Install Go dependencies
make dry-run       # Test CSV parsing & JSON serialization
```

**Output**:
```
‚úÖ Total Records Parsed: 473
‚è±Ô∏è  Parse Time: 270¬µs
üöÄ Parse Rate: 1,755,102 records/sec
```

### Run Unit Tests
```bash
make test          # Run all tests
make test-coverage # Generate coverage report
```

**Results**:
- ‚úÖ 5/5 tests passed
- ‚úÖ `models/detection.go`: 100% coverage
- ‚úÖ `ingestion/csv_reader.go`: 46.8% coverage

### Production Usage (Requires Kafka)
```bash
# Configure Kafka credentials
cp .env.example .env
# Edit .env with Confluent Cloud settings

# Stream mode (recommended for large datasets)
make run-stream

# Or with custom parameters
./bin/producer \
  -csv ../local-mvp/traffic_signs.csv \
  -vehicle vehicle-001 \
  -workers 20 \
  -session session-$(date +%Y%m%d)
```

---

## Architecture

```
CSV File ‚Üí CSV Reader ‚Üí Channel ‚Üí Worker Pool (goroutines) ‚Üí Kafka Producer ‚Üí Confluent Cloud
           (stream)      (1000)   (10-50 workers)            (batching)
```

**Key Components**:
- `ingestion/csv_reader.go` - Streams CSV records to channel
- `producer/kafka_producer.go` - Thread-safe producer with retry logic
- `models/detection.go` - Data model & JSON serialization
- `cmd/producer/main.go` - CLI application
- `cmd/test-dryrun/main.go` - Test without Kafka

---

## Configuration

### CLI Flags
| Flag | Description | Default |
|------|-------------|---------|
| `-csv` | Path to CSV file | `../local-mvp/traffic_signs.csv` |
| `-vehicle` | Vehicle ID | `vehicle-001` |
| `-session` | Session ID (auto-generated if empty) | UUID |
| `-workers` | Concurrent goroutines | `10` |
| `-batch` | Batch mode (load all to memory) | `false` |

### Environment Variables (.env)
```bash
KAFKA_BOOTSTRAP_SERVERS=pkc-xxxxx.us-east-1.aws.confluent.cloud:9092
KAFKA_SASL_USERNAME=your-api-key
KAFKA_SASL_PASSWORD=your-api-secret
KAFKA_TOPIC=traffic-sign-detections
KAFKA_COMPRESSION_TYPE=snappy
KAFKA_BATCH_SIZE=16384
```

---

## Testing

### Dry-Run (No Kafka)
```bash
make dry-run       # Parse CSV, show 5 JSON samples
make dry-run-full  # Show all 473 records
```

### Unit Tests
```bash
make test          # Run tests
make test-coverage # Generate coverage.html
```

**Tested Scenarios**:
- CSV parsing with valid/invalid data
- JSON serialization/deserialization
- Null field handling
- GPS data enrichment
- UUID generation

### Validation Checklist
- [x] CSV parsing (473 records from local-mvp)
- [x] Data enrichment (UUIDs, timestamps, IDs)
- [x] JSON serialization
- [x] Error handling (malformed rows)
- [ ] Kafka integration (requires .env setup)
- [ ] TPS benchmarking with 256GB dataset

---

## Performance

### Expected Metrics
- **Throughput**: 5K-15K messages/sec
- **Parse Rate**: 1.7M records/sec (dry-run)
- **Latency (P99)**: <100ms with retry
- **Memory**: ~50MB for 10 workers

### Monitoring Output
```
üìä Metrics - Sent: 47300 | Acked: 47250 | Failed: 12 | Pending: 38
‚è±Ô∏è  Total Time: 5.2s
üöÄ Throughput: 9087.32 messages/sec
‚úÖ Success Rate: 99.97%
```

---

## Error Handling

### Retriable (with exponential backoff)
- Network timeouts
- Broker unavailability
- Throttling

**Retry**: 100ms ‚Üí 200ms ‚Üí 400ms ‚Üí 800ms ‚Üí 1.6s (max 5 attempts)

### Non-Retriable (skip & log)
- Invalid message format
- Authorization failures
- Topic not found

---

## Design Decisions

**Why Golang?**
- Native goroutines for concurrency
- Low GC overhead for high throughput
- Mature Kafka client (`confluent-kafka-go`)

**Why Channels?**
- Decouples CSV reading from Kafka sending
- Buffering prevents memory overflow
- Easy worker scaling

**Why Idempotence?**
- Prevents duplicate records on retries
- Safe for downstream analytics

---

## Project Structure

```
prod-pipeline/
‚îú‚îÄ‚îÄ cmd/
‚îÇ   ‚îú‚îÄ‚îÄ producer/main.go        # Main CLI application
‚îÇ   ‚îî‚îÄ‚îÄ test-dryrun/main.go     # Test without Kafka
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ kafka.go                # Kafka configuration
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ detection.go            # Data model
‚îÇ   ‚îî‚îÄ‚îÄ detection_test.go       # Unit tests (100% coverage)
‚îú‚îÄ‚îÄ ingestion/
‚îÇ   ‚îú‚îÄ‚îÄ csv_reader.go           # CSV streaming parser
‚îÇ   ‚îî‚îÄ‚îÄ csv_reader_test.go      # Unit tests
‚îú‚îÄ‚îÄ producer/
‚îÇ   ‚îî‚îÄ‚îÄ kafka_producer.go       # Thread-safe Kafka producer
‚îú‚îÄ‚îÄ Makefile                    # Build & test commands
‚îú‚îÄ‚îÄ go.mod                      # Go dependencies
‚îî‚îÄ‚îÄ README.md                   # This file
```

---

## Next Steps

1. **Configure Kafka**: Set up Confluent Cloud credentials in `.env`
2. **Test Integration**: Run `make run-stream` to test actual Kafka ingestion
3. **Benchmark**: Measure TPS with larger datasets (10K, 100K, 500K records)
4. **GPS Enrichment**: Add real-time vehicle positioning data
5. **Snowflake Integration**: Set up Kafka Connect for warehouse ingestion

---

## Related Documentation

- [Production Pipeline Specs](../docs/prod-pipeline-specs-en.md)
- [Local MVP README](../local-mvp/README.md)
- [Confluent Kafka Go Docs](https://docs.confluent.io/kafka-clients/go/current/overview.html)
- [Main Project README](../README.md)
